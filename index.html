<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="End-to-end Robotic Programming with Augmented Reality ">
  <meta name="keywords" content="">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>RAMPA: Robotic Augmented Reality for Machine Programming by Demonstration</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <!-- <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script> -->

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">RAMPA: Robotic Augmented Reality for Machine Programming by Demonstration</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a target="_blank">Fatih Doğangün</a><sup>1</sup>,</span>
            <span class="author-block">
              <a target="_blank">Serdar Bahar</a><sup>1</sup>,</span>
            <span class="author-block">
              <a target="_blank" >Yiğit Yıldırım</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a target="_blank" >Bora Toprak Temir</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a target="_blank" >Emre Uğur</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a target="_blank" >Mustafa Doğa Doğan</a><sup>2,1</sup>,
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup> Department of Computer Engineering, Bogazici University &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <sup>2</sup> Adobe Research, Basel, Switzerland</span>
            <br/><br/>
            <!-- <span class="author-block"><a href="https://uist.acm.org/2024/" target="_blank" >2024 ACM Symposium on User Interface Software and Technology (UIST)</a></span> -->
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/abs/2410.13412" target="_blank" 
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/dogadogan/rampa" target="_blank" 
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
           
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

 <!-- Abstract. -->
 <div class="columns is-centered has-text-centered">
  <div class="column is-four-fifths">
    <h2 class="title is-3">Abstract</h2>
    <div class="content has-text-justified">
      <p> Abstract—As robotics continue to enter various sectors beyond
      traditional industrial applications, the need for intuitive robot
      training and interaction systems becomes increasingly more
      important. This paper introduces Robotic Augmented Reality
      for Machine Programming (<span class="dnerf">Rampa</span>), a system that utilizes the
      capabilities of state-of-the-art and commercially available AR
      headsets, e.g., <span class="italic">Meta Quest</span> 3, to facilitate the application of Programming from Demonstration 
      (PbD) approaches on industrial
      robotic arms, such as <span class="italic">Universal Robots UR10</span>. Our approach
      enables in-situ data recording, visualization, and fine-tuning of
      skill demonstrations directly within the user’s physical environment. <span class="dnerf">Rampa</span> 
      addresses critical challenges of PbD, such as safety
      concerns, programming barriers, and the inefficiency of collecting
      demonstrations on the actual hardware. The performance of our
      system is evaluated against the traditional method of kinesthetic
      control in teaching three different robotic manipulation tasks and
      analyzed with quantitative metrics, measuring task performance
      and completion time, trajectory smoothness, system usability,
      user experience, and task load using standardized surveys. Our
      findings indicate a substantial advancement in how robotic tasks
      are taught and refined, promising improvements in operational
      safety, efficiency, and user engagement in robotic programming. </p>
     
    </div>
  </div>
</div>
<!--/ Abstract. -->


<section class="section">
  <!-- Paper video. -->
  <div class="columns is-centered has-text-centered">
    <div class="column is-four-fifths" >
      <h2 class="title is-3">Video</h2>
        <video id="teaser" autoplay loop playsinline controls height="300px">
          <source src="./static/videos/rampa_video.mp4"
                  type="video/mp4">
        </video>
    </div>
  </div>
  <!--/ Paper video. -->

</section>



<section class="hero is-light is-small">
  <div class="hero-body">
    <div class="container">
      <div class="columns is-centered">
        <div class="column is-8">
          <div class="content">
            <h2 class="title is-3">Key Features of <span class="dnerf">Rampa</span> </h2>
            <img src="./static/images/main_figure.png"
        class="interpolation-image" width="100%"/>
          </div>
         <h3 class="title is-4">Data Recording</h3>
            <p> Users use AR to demonstrate skills, capturing the data directly in the user's environment, using hand pinch, as shown in the figure above.
              Compared to traditional methods, such as the kinesthetic control, <span class="dnerf">RAMPA</span> simplifies the collection of 
              demonstration trajectories, an integral component of PbD procedures, offering a more efficient experience for 
              robotic programmers.
            </p>
        <h3 class="title is-4">Data Playback and Adjustment</h3>
          <p> <img  style="float: left;padding-right: 30px;" src="./static/images/traj_playback.png" width="450px"/>
            <br/><br/>
            The system allows users to visualize and modify the movements of a virtual robot before these movements are executed 
            on the actual robotic arm. This step ensures that potential errors can be corrected in a safe, virtual space. 
            Moreover, the ability to adjust any portion of demonstration trajectories increases the operator's overall performance.
          </p><br/><br/><br/><br/>

        <h3 class="title is-4">ML Training and Preview</h3>
          <p> <img  style="float: left;padding-right: 30px;" src="./static/images/ml_overview.png" width="450px"/>
            <br/><br/><br/><br/>
            Utilizing the collected data, popular PbD models, such as Probabilistic Movement Primitives (ProMPs), can be trained and validated
            within the AR environment. Users can observe the generated trajectories in real-time, ensuring the robot's behavior aligns with desired outcomes.
          </p><br/>
         </div>
        </div>
      </div>  
    </div>
  </div>
</div>
</section>

<!--
<section class="section">
  
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Implementation</h2>

        <div class="content has-text-justified">
          
          <p>
            XR-Objects  leverages developments in spatial understanding via tools such as  SLAM, 
            available in <a
            href="https://developers.google.com/ar">Google ARCore </a> and 
            <a href="https://developer.apple.com/augmented-reality/arkit">Apple ARKit</a>, 
            and machine learning models for object segmentation and classification (<a href="https://cocodataset.org/COCO">COCO</a> 
            via <a href="https://developers.google.com/mediapipe">MediaPipe</a>), that enable us to implement AR interactions 
            with semantic depth.
            We also integrate a Multimodal Large Language Model (MLLM), 
            <a href="https://deepmind.google/technologies/gemini/">Google Gemini</a>, into our system, 
            which further enhances our ability to automate the recognition of objects and their specific 
            semantic information within XR spaces.  
          </p>
        
          <img poster="" id="fullbody" src="./static/images/pipeline_new.png">
        </div>
      </div>
    </div>
  </div>
</section>
	
-->

<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@misc{dogangun2024rampa,
      title={RAMPA: Robotic Augmented Reality for Machine Programming and Automation},
      author={Dogangun, Fatih and Bahar, Serdar and Yildirim, Yigit and Temir, Bora Toprak and Ugur, Emre and Dogan, Mustafa Doga},
      howpublished={arXiv:2410.13412, 2024}
}
</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is built on top of the original<a
            href="https://github.com/nerfies/nerfies.github.io">  Nerfies, <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 </a> International License.
          </p>
          </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
