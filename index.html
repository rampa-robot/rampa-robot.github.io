<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="End-to-end Robotic Programming with Augmented Reality ">
  <meta name="keywords" content="">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>RAMPA: Robotic Augmented Reality for Machine Programming and Automation</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <!-- <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script> -->

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">RAMPA: Robotic Augmented Reality for Machine Programming and Automation</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a target="_blank">Fatih Doğangün</a><sup>1</sup>,</span>
            <span class="author-block">
              <a target="_blank">Serdar Bahar</a><sup>1</sup>,</span>
            <span class="author-block">
              <a target="_blank" >Yiğit Yıldırım</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a target="_blank" >Bora Toprak Temir</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a target="_blank" >Emre Uğur</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a target="_blank" >Mustafa Doğa Doğan</a><sup>2,1</sup>,
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup> Department of Computer Engineering, Bogazici University &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <sup>2</sup> Adobe Research, Basel, Switzerland</span>
            <br/><br/>
            <!-- <span class="author-block"><a href="https://uist.acm.org/2024/" target="_blank" >2024 ACM Symposium on User Interface Software and Technology (UIST)</a></span> -->
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/abs/2410.13412" target="_blank" 
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/dogadogan/rampa" target="_blank" 
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
           
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

 <!-- Abstract. -->
 <div class="columns is-centered has-text-centered">
  <div class="column is-four-fifths">
    <h2 class="title is-3">Abstract</h2>
    <div class="content has-text-justified">
      <p>
	Abstract—As robotics continue to enter various sectors beyond
	traditional industrial applications, the need for intuitive robot
	training and interaction systems becomes increasingly more
	important. This paper introduces Robotic Augmented Reality
	for Machine Programming (<span class="dnerf">RAMPA</span>), a system that utilizes the
	capabilities of state-of-the-art and commercially available AR
	headsets, e.g., <span class="italic">Meta Quest</span> 3, to facilitate the application of Pro-
	gramming from Demonstration (<span class="italic">PbD</span>) approaches on industrial
	robotic arms, such as Universal Robots UR10. Our approach
	enables in-situ data recording, visualization, and fine-tuning of
	skill demonstrations directly within the user’s physical environ-
	ment. <span class="dnerf">RAMPA</span> addresses critical challenges of <span class="italic">PbD</span>, such as safety
	concerns, programming barriers, and the inefficiency of collecting
	demonstrations on the actual hardware. The performance of our
	system is evaluated against the traditional method of kinesthetic
	control in teaching three different robotic manipulation tasks and
	analyzed with quantitative metrics, measuring task performance
	and completion time, trajectory smoothness, system usability,
	user experience, and task load using standardized surveys. Our
	findings indicate a substantial advancement in how robotic tasks
	are taught and refined, promising improvements in operational
	safety, efficiency, and user engagement in robotic programming.
      </p>
     
    </div>
  </div>
</div>
<!--/ Abstract. -->


<section class="section">
  <!-- Paper video. -->
  <div class="columns is-centered has-text-centered">
    <div class="column is-four-fifths" >
      <h2 class="title is-3">Video</h2>
        <video id="teaser" autoplay loop playsinline controls height="300px">
          <source src="./static/videos/rampa_video.mp4"
                  type="video/mp4">
        </video>
    </div>
  </div>
  <!--/ Paper video. -->

</section>


<!--
<section class="hero is-light is-small">
  <div class="hero-body">
    <div class="container">
      <div class="columns is-centered">
        <div class="column is-8">
          <div class="content">
            <h2 class="title is-3">Applications</h2>
            <img src="https://github.com/google/xr-objects/raw/main/docs/FigureXRObjects.jpg"
        class="interpolation-image" width="100%"/>
        </div>
            <p> <img  style="float: left;padding-right: 30px;" src="./static/images/applications.png"
              width="225px"/>Through AOI, we envision XR-Objects to be useful across a variety of real-world applications. 
              By enabling in situ digital interactions with non-instrumented analog objects, we can expand their utility 
              (e.g., enabling a pot to double as a cooking timer), better synthesize their relevant information 
              (e.g., comparing nutritional value), and overall enable richer interactivity and flexibility in everyday interactions. 
                Here we present five example application scenarios from a broad application space we envision that highlight the value 
                of XR-Objects.</p><br/>
          </div>
        </div>
      </div>  
    </div>
        
      <br/>
        <div id="results-carousel" class="carousel results-carousel" style="border-width:0px;">
         
          <div class="item item-steve" style="border-width:0px;">
            <div class="columns is-centered" style="border-width:0px;">
            <img poster="" id="steve"  src="./static/images/discover.png"
            width="70%"/>
          </div>
          </div>
          
          <div class="item item-shiba" style="border-width:0px;">
            <div class="columns is-centered" style="border-width:0px;">
              
            <img poster="" id="shiba" src="./static/images/productivity.png"
            width="80%">
          </div>
          </div>
          <div class="item item-chair-tp" style="border-width:0px;">
            <div class="item" style="border-width:0px;">
              <img poster="" id="chair-tp"  src="./static/images/learning.png"
              width="90%">
            </div>
          </div>
          <div class="item item-fullbody" style="border-width:0px;">
            <img poster="" id="fullbody" src="./static/images/IOT.png"
            width="95%">
          </div>      
        </div>
        <br/>
        <h2 class="subtitle has-text-centered">
          <span class="dnerf">XR-Objects</span> makes analog objects interactable as if they were digital.
        </h2>
      
  </div>
 
</div>
</section>


<section class="section">
  
  <div class="container is-max-desktop">
    <!-- Concurrent Work.  -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Implementation</h2>

        <div class="content has-text-justified">
          
          <p>
            XR-Objects  leverages developments in spatial understanding via tools such as  SLAM, 
            available in <a
            href="https://developers.google.com/ar">Google ARCore </a> and 
            <a href="https://developer.apple.com/augmented-reality/arkit">Apple ARKit</a>, 
            and machine learning models for object segmentation and classification (<a href="https://cocodataset.org/COCO">COCO</a> 
            via <a href="https://developers.google.com/mediapipe">MediaPipe</a>), that enable us to implement AR interactions 
            with semantic depth.
            We also integrate a Multimodal Large Language Model (MLLM), 
            <a href="https://deepmind.google/technologies/gemini/">Google Gemini</a>, into our system, 
            which further enhances our ability to automate the recognition of objects and their specific 
            semantic information within XR spaces.  
          </p>
        
          <img poster="" id="fullbody" src="./static/images/pipeline_new.png">
        </div>
      </div>
    </div>
    <!--/ Concurrent Work. -->

  </div>
</section>
	
-->

<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@misc{dogangun2024ramparoboticaugmentedreality,
      title={RAMPA: Robotic Augmented Reality for Machine Programming and Automation}, 
      author={Fatih Dogangun and Serdar Bahar and Yigit Yildirim and Bora Toprak Temir and Emre Ugur and Mustafa Doga Dogan},
      year={2024},
      eprint={2410.13412},
      archivePrefix={arXiv},
      primaryClass={cs.RO},
      url={https://arxiv.org/abs/2410.13412}, 
}
}</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is built on top of the original<a
            href="https://github.com/nerfies/nerfies.github.io">  Nerfies, <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 </a> International License.
          </p>
          </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
